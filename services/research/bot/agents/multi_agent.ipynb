{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_community/document_loaders/blob_loaders/file_system.py:5: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.document_loaders.blob_loaders.schema import Blob, BlobLoader\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_community/document_loaders/__init__.py:219: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.document_loaders.youtube import (\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:775: UserWarning: Mixing V1 models and V2 models (or constructs, like `TypeAdapter`) is not supported. Please upgrade `TextRequestsWrapper` to V2.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader,TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from openai import OpenAI\n",
    "import requests\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "    format_to_openai_tool_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "from typing import Literal\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import MessagesState, END\n",
    "from langgraph.types import Command\n",
    "\n",
    "from typing import Literal\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import List, Union, Dict, Optional\n",
    "from pinecone import Index\n",
    "\n",
    "import validators\n",
    "import requests\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_API_ENV = os.getenv(\"PINECONE_API_ENV\")\n",
    "PINECONE_QA_BASE_INDEX_NAME = os.getenv(\"PINECONE_QA_BASE_INDEX_NAME\")\n",
    "PINECONE_KNOWLEDGE_BASE_INDEX_NAME = os.getenv(\"PINECONE_KNOWLEDGE_BASE_INDEX_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openAI_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "embedding_model = openAI_client.embeddings\n",
    "pc = Pinecone(api_key = PINECONE_API_KEY, environment = PINECONE_API_ENV)\n",
    "doc_index = pc.Index(PINECONE_KNOWLEDGE_BASE_INDEX_NAME)\n",
    "qa_index = pc.Index(PINECONE_QA_BASE_INDEX_NAME)\n",
    "\n",
    "\n",
    "def get_embedding(text) :\n",
    "    \"\"\"\n",
    "        Function to convert the text string into embeddings using text-embedding-3-small from OpenAI\n",
    "    \n",
    "        Args:\n",
    "            text : A string which will contain either the text chunk or the user query\n",
    "            \n",
    "        Returns:\n",
    "            vector : A vector of 1536 dimensions\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = embedding_model.create(\n",
    "            input=text,\n",
    "            model=\"text-embedding-3-small\"\n",
    "        )\n",
    "        \n",
    "        return response.data[0].embedding   \n",
    "    \n",
    "    except Exception as e:\n",
    "        raise Exception(str(e))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents and Tools Setup\n",
    "Here, we need the worker agents and tools setup for this to work and we will be using the following : \n",
    "\n",
    "- Text Knowledge Base Agent\n",
    "- Q&A Knowledge Base Agent\n",
    "- Fallback Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Knowledge Base Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_query_prompt_template = \"\"\"\n",
    "    \\n\\n User Query : {query}\n",
    "    \\n\\n Documents : {documents}\n",
    "\"\"\"\n",
    "\n",
    "kb_query_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\",\"documents\"],\n",
    "    template= kb_query_prompt_template\n",
    ")\n",
    "\n",
    "kb_system_instructions = \"\"\"\n",
    "    You are a specialised AI document analyser working at an edtech startup, and you will be assisting the users to answer their queries. You will be given \n",
    "    the top relevant documents and you have to use those to answer the query asked by the user, which will be given to you below. \n",
    "    \n",
    "    In the relevant documents,you will be given the cosine similarity score, the document name and the reference (which is the page number where this \n",
    "    text was in the document) and the text itself. You can in you answer integrate the document name and reference to build authenticity of your answer, \n",
    "    by precisely writing it like (reference page : page_num). You should eventually tell the user to explore more about the topic in the document and that \n",
    "    reference page. Infact, this would be highly favourable if you mention the document name and reference to build your authenticity\n",
    "    \n",
    "    MAKE SURE YOU DO NOT ANSWER FROM ANYTHING APART FROM THE DOCUMENTS GIVEN TO YOU. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_openAI_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "kb_text_assistant_id = os.getenv(\"KNOWLEDGE_BASE_OPENAI_ASSISTANT_ID\")\n",
    "\n",
    "kb_doc_thread = kb_openAI_client.beta.threads.create()\n",
    "\n",
    "def kb_fetch_answer(final_prompt):\n",
    "    kb_openAI_client.beta.threads.messages.create(\n",
    "        thread_id=kb_doc_thread.id,\n",
    "        role=\"user\",\n",
    "        content= [{\n",
    "            \"type\" : \"text\",\n",
    "            \"text\" : final_prompt\n",
    "        }]\n",
    "    )\n",
    "            \n",
    "    run = openAI_client.beta.threads.runs.create(\n",
    "        thread_id=kb_doc_thread.id,\n",
    "        assistant_id=kb_text_assistant_id,\n",
    "    )\n",
    "    \n",
    "    while True:\n",
    "        run = openAI_client.beta.threads.runs.retrieve(\n",
    "            thread_id=kb_doc_thread.id,\n",
    "            run_id=run.id\n",
    "        )\n",
    "        if run.status == 'completed':\n",
    "            messages = openAI_client.beta.threads.messages.list(\n",
    "                thread_id=kb_doc_thread.id\n",
    "            )\n",
    "            return messages.data[0].content[0].text.value\n",
    "        elif run.status in ['failed', 'cancelled', 'expired']:\n",
    "            raise Exception(f\"Run ended with status: {run.status}\")\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def kb_answer_query(query):\n",
    "    \"\"\"\n",
    "        Find relevant documents for a given query and userID.\n",
    "        \n",
    "        Args:\n",
    "        - query: The search query\n",
    "        \n",
    "        Returns:\n",
    "        Final text answer from the LLM, given the relevant documents.\n",
    "    \"\"\"\n",
    "    query_vector = get_embedding(query)\n",
    "    results = doc_index.query(\n",
    "        vector=query_vector,\n",
    "        top_k=5,\n",
    "        include_values=False,\n",
    "        include_metadata=True,\n",
    "        filter={\n",
    "            \"userID\": os.getenv(\"ADMIN_ID\")\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    relevant_texts = []\n",
    "    for record in results['matches']:\n",
    "        text = {\n",
    "            'score': record['score'],\n",
    "            'text': record['metadata']['chunk'],\n",
    "            'name': record['metadata']['document_name'],\n",
    "            'reference': int(record[\"metadata\"][\"page_number\"]) + 1\n",
    "        }\n",
    "        relevant_texts.append(text)\n",
    "    \n",
    "    final_prompt = kb_query_prompt.invoke({\"query\" : query,\"documents\": relevant_texts}).text\n",
    "    return kb_fetch_answer(final_prompt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new None chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `kb_answer_query` with `{'query': 'what is meant by rolling up the state?'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\"Rolling up the state\" refers to the practice of lifting state up to a common ancestor component in a React application. As an application grows, multiple components might need access to the same piece of state. Instead of duplicating this state across different components, you can manage it in the least common ancestor (LCA) of these components. This approach helps in avoiding unoptimal re-renders by ensuring that only components that need to know about the state changes are re-rendered (W9-react, reference page: 37).\n",
      "\n",
      "To explore more about this concept, you can refer to the document \"W9-react\" on page 37.\u001b[0m\u001b[32;1m\u001b[1;3m\"Rolling up the state\" refers to the practice of lifting state up to a common ancestor component in a React application. As an application grows, multiple components might need access to the same piece of state. Instead of duplicating this state across different components, you can manage it in the least common ancestor (LCA) of these components. This approach helps in avoiding unoptimal re-renders by ensuring that only components that need to know about the state changes are re-rendered.\n",
      "\n",
      "To explore more about this concept, you can refer to the document \"W9-react\" on page 37.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'actions': [ToolAgentAction(tool='kb_answer_query', tool_input={'query': 'what is meant by rolling up the state?'}, log=\"\\nInvoking: `kb_answer_query` with `{'query': 'what is meant by rolling up the state?'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_fb6oDw2H9X2Dshatg2e9uTYT', 'function': {'arguments': '{\"query\":\"what is meant by rolling up the state?\"}', 'name': 'kb_answer_query'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_4691090a87'}, id='run-fcccfc5e-908f-442e-86ac-756ae3fcb9f1', tool_calls=[{'name': 'kb_answer_query', 'args': {'query': 'what is meant by rolling up the state?'}, 'id': 'call_fb6oDw2H9X2Dshatg2e9uTYT', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'kb_answer_query', 'args': '{\"query\":\"what is meant by rolling up the state?\"}', 'id': 'call_fb6oDw2H9X2Dshatg2e9uTYT', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_fb6oDw2H9X2Dshatg2e9uTYT')],\n",
       "  'messages': [AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_fb6oDw2H9X2Dshatg2e9uTYT', 'function': {'arguments': '{\"query\":\"what is meant by rolling up the state?\"}', 'name': 'kb_answer_query'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_4691090a87'}, id='run-fcccfc5e-908f-442e-86ac-756ae3fcb9f1', tool_calls=[{'name': 'kb_answer_query', 'args': {'query': 'what is meant by rolling up the state?'}, 'id': 'call_fb6oDw2H9X2Dshatg2e9uTYT', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'kb_answer_query', 'args': '{\"query\":\"what is meant by rolling up the state?\"}', 'id': 'call_fb6oDw2H9X2Dshatg2e9uTYT', 'index': 0, 'type': 'tool_call_chunk'}])]},\n",
       " {'steps': [AgentStep(action=ToolAgentAction(tool='kb_answer_query', tool_input={'query': 'what is meant by rolling up the state?'}, log=\"\\nInvoking: `kb_answer_query` with `{'query': 'what is meant by rolling up the state?'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_fb6oDw2H9X2Dshatg2e9uTYT', 'function': {'arguments': '{\"query\":\"what is meant by rolling up the state?\"}', 'name': 'kb_answer_query'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_4691090a87'}, id='run-fcccfc5e-908f-442e-86ac-756ae3fcb9f1', tool_calls=[{'name': 'kb_answer_query', 'args': {'query': 'what is meant by rolling up the state?'}, 'id': 'call_fb6oDw2H9X2Dshatg2e9uTYT', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'kb_answer_query', 'args': '{\"query\":\"what is meant by rolling up the state?\"}', 'id': 'call_fb6oDw2H9X2Dshatg2e9uTYT', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_fb6oDw2H9X2Dshatg2e9uTYT'), observation='\"Rolling up the state\" refers to the practice of lifting state up to a common ancestor component in a React application. As an application grows, multiple components might need access to the same piece of state. Instead of duplicating this state across different components, you can manage it in the least common ancestor (LCA) of these components. This approach helps in avoiding unoptimal re-renders by ensuring that only components that need to know about the state changes are re-rendered (W9-react, reference page: 37).\\n\\nTo explore more about this concept, you can refer to the document \"W9-react\" on page 37.')],\n",
       "  'messages': [FunctionMessage(content='\"Rolling up the state\" refers to the practice of lifting state up to a common ancestor component in a React application. As an application grows, multiple components might need access to the same piece of state. Instead of duplicating this state across different components, you can manage it in the least common ancestor (LCA) of these components. This approach helps in avoiding unoptimal re-renders by ensuring that only components that need to know about the state changes are re-rendered (W9-react, reference page: 37).\\n\\nTo explore more about this concept, you can refer to the document \"W9-react\" on page 37.', additional_kwargs={}, response_metadata={}, name='kb_answer_query')]},\n",
       " {'output': '\"Rolling up the state\" refers to the practice of lifting state up to a common ancestor component in a React application. As an application grows, multiple components might need access to the same piece of state. Instead of duplicating this state across different components, you can manage it in the least common ancestor (LCA) of these components. This approach helps in avoiding unoptimal re-renders by ensuring that only components that need to know about the state changes are re-rendered.\\n\\nTo explore more about this concept, you can refer to the document \"W9-react\" on page 37.',\n",
       "  'messages': [AIMessage(content='\"Rolling up the state\" refers to the practice of lifting state up to a common ancestor component in a React application. As an application grows, multiple components might need access to the same piece of state. Instead of duplicating this state across different components, you can manage it in the least common ancestor (LCA) of these components. This approach helps in avoiding unoptimal re-renders by ensuring that only components that need to know about the state changes are re-rendered.\\n\\nTo explore more about this concept, you can refer to the document \"W9-react\" on page 37.', additional_kwargs={}, response_metadata={})]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb_main_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an assistant who can handle queries by using the tool you have and refer the user to correct material, you just have to pass the exact user query to the tool and it will do the rest, If the tool does not give relevant answers, just tell the user that you do not know by giving the response as 'I do not know'...\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "kb_llm = ChatOpenAI(\n",
    "    api_key = OPENAI_API_KEY,\n",
    "    model = \"gpt-4o\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "kb_tools = [kb_answer_query]\n",
    "kb_llm_with_tools = kb_llm.bind_tools(kb_tools)\n",
    "\n",
    "text_knowledge_base_agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | kb_main_prompt\n",
    "    | kb_llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")\n",
    "\n",
    "text_knowledge_base_agent_agent_executor = AgentExecutor(agent=text_knowledge_base_agent, tools=kb_tools, verbose=True)\n",
    "list(text_knowledge_base_agent_agent_executor.stream({\"input\": \"what is meant by rolling up the state?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A Knowledge Base Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instructions_qa = \"\"\"\n",
    "    You are a specialised AI context aware doubt solver working at an edtech startup, and you will be assisting the users to answer their queries based on previous intrcutors and TAs solved queries.\n",
    "    You will be given a query by the user and the top relevant documents and you have to use those to answer the query asked by the user, which will be given to you below. \n",
    "    In the relevant documents,you will be given the id of the conversation, the cosine similarity score, the question which was aksed by previous student and the answers by the TAs, along with the id of the QA pair. \n",
    "    YOU MUST tell the user that they can explore this further by going to that thread (give them the id) and looking at the entire conversation for better understanding. (Think of this as a reference to build authenticity, as you mention the id).\n",
    "    \n",
    "    MAKE SURE YOU DO NOT ANSWER FROM ANYTHING APART FROM THE DOCUMENTS GIVEN TO YOU. \n",
    "\"\"\"\n",
    "\n",
    "query_prompt_template_qa = \"\"\"\n",
    "    \\n\\n User Query : {query}\n",
    "    \\n\\n Documents : {documents}\n",
    "\"\"\"\n",
    "\n",
    "query_prompt_qa = PromptTemplate(\n",
    "    input_variables=[\"query\",\"documents\"],\n",
    "    template=query_prompt_template_qa\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_assistant_id = os.getenv(\"QA_OPENAI_ASSISTANT_ID\")\n",
    "\n",
    "qa_thread = openAI_client.beta.threads.create()\n",
    "\n",
    "def fetch_answer_qa(final_prompt):\n",
    "    openAI_client.beta.threads.messages.create(\n",
    "        thread_id=qa_thread.id,\n",
    "        role=\"user\",\n",
    "        content= [{\n",
    "            \"type\" : \"text\",\n",
    "            \"text\" : final_prompt\n",
    "        }]\n",
    "    )\n",
    "            \n",
    "    run = openAI_client.beta.threads.runs.create(\n",
    "        thread_id=qa_thread.id,\n",
    "        assistant_id=qa_assistant_id,\n",
    "    )\n",
    "    \n",
    "    while True:\n",
    "        run = openAI_client.beta.threads.runs.retrieve(\n",
    "            thread_id=qa_thread.id,\n",
    "            run_id=run.id\n",
    "        )\n",
    "        if run.status == 'completed':\n",
    "            messages = openAI_client.beta.threads.messages.list(\n",
    "                thread_id=qa_thread.id\n",
    "            )\n",
    "            return messages.data[0].content[0].text.value\n",
    "        elif run.status in ['failed', 'cancelled', 'expired']:\n",
    "            raise Exception(f\"Run ended with status: {run.status}\")\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def answer_query_qa(query):\n",
    "    \"\"\"\n",
    "        Find relevant documents for a given query and userID.\n",
    "        \n",
    "        Args:\n",
    "        - query: The search query\n",
    "        \n",
    "        Returns:\n",
    "        Final text answer from the LLM, given the relevant documents.\n",
    "    \"\"\"\n",
    "    query_vector = get_embedding(query)\n",
    "    \n",
    "    results = qa_index.query(\n",
    "        vector = query_vector,\n",
    "        top_k = 10,\n",
    "        include_values = False,\n",
    "        include_metadata = True,\n",
    "        filter={\n",
    "            \"ADMIN_ID\" : os.getenv('ADMIN_ID_QA'),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    relevant_pairs = []\n",
    "    for record in results['matches']:\n",
    "        pair = {}\n",
    "        pair['id'] = record['metadata']['QA_ID']\n",
    "        pair['score'] = record['score']\n",
    "        pair['question'] = record['metadata']['question']\n",
    "        pair['answer'] = record['metadata']['answer']\n",
    "        relevant_pairs.append(pair)\n",
    "    \n",
    "    \n",
    "    final_prompt = query_prompt_qa.invoke({\"query\" : query,\"documents\": relevant_pairs}).text\n",
    "    return fetch_answer_qa(final_prompt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new None chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `answer_query_qa` with `{'query': 'hydration error in NEXTJS when animating dropdown menu'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mTo address the hydration error you're experiencing in your Next.js project while animating a dropdown menu, you can follow these steps based on a similar issue resolved in a previous discussion:\n",
      "\n",
      "1. **Restart the Server**: Sometimes, simply restarting the Next.js server can resolve hydration issues.\n",
      "\n",
      "2. **Inspect Styles**: Use the browser's inspect element tool to check if Tailwind CSS (or any other CSS framework you're using) is being loaded correctly. If the styles are present but not applied, it might indicate a configuration issue.\n",
      "\n",
      "3. **Verify Tailwind CSS Setup**: Ensure that Tailwind CSS is set up correctly in your project. This involves checking your Tailwind configuration file to make sure the necessary classes are being generated and applied. Refer to the Tailwind CSS documentation for detailed setup instructions.\n",
      "\n",
      "For more detailed insights, you can explore the conversation with the ID `9cef8bde-c6a7-4758-a736-7d2097c0b38a` to understand how a similar issue was resolved.\u001b[0m\u001b[32;1m\u001b[1;3mTo address the hydration error you're experiencing in your Next.js project while animating a dropdown menu, you can follow these steps:\n",
      "\n",
      "1. **Restart the Server**: Sometimes, simply restarting the Next.js server can resolve hydration issues.\n",
      "\n",
      "2. **Inspect Styles**: Use the browser's inspect element tool to check if Tailwind CSS (or any other CSS framework you're using) is being loaded correctly. If the styles are present but not applied, it might indicate a configuration issue.\n",
      "\n",
      "3. **Verify Tailwind CSS Setup**: Ensure that Tailwind CSS is set up correctly in your project. This involves checking your Tailwind configuration file to make sure the necessary classes are being generated and applied. Refer to the Tailwind CSS documentation for detailed setup instructions.\n",
      "\n",
      "For more detailed insights, you can explore the conversation with the ID `9cef8bde-c6a7-4758-a736-7d2097c0b38a` to understand how a similar issue was resolved.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'actions': [ToolAgentAction(tool='answer_query_qa', tool_input={'query': 'hydration error in NEXTJS when animating dropdown menu'}, log=\"\\nInvoking: `answer_query_qa` with `{'query': 'hydration error in NEXTJS when animating dropdown menu'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_VUUTA6T5xKkKIEoBgb14CUIf', 'function': {'arguments': '{\"query\":\"hydration error in NEXTJS when animating dropdown menu\"}', 'name': 'answer_query_qa'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4'}, id='run-156dffa9-3b9c-4367-8408-d433913034c0', tool_calls=[{'name': 'answer_query_qa', 'args': {'query': 'hydration error in NEXTJS when animating dropdown menu'}, 'id': 'call_VUUTA6T5xKkKIEoBgb14CUIf', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'answer_query_qa', 'args': '{\"query\":\"hydration error in NEXTJS when animating dropdown menu\"}', 'id': 'call_VUUTA6T5xKkKIEoBgb14CUIf', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_VUUTA6T5xKkKIEoBgb14CUIf')],\n",
       "  'messages': [AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_VUUTA6T5xKkKIEoBgb14CUIf', 'function': {'arguments': '{\"query\":\"hydration error in NEXTJS when animating dropdown menu\"}', 'name': 'answer_query_qa'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4'}, id='run-156dffa9-3b9c-4367-8408-d433913034c0', tool_calls=[{'name': 'answer_query_qa', 'args': {'query': 'hydration error in NEXTJS when animating dropdown menu'}, 'id': 'call_VUUTA6T5xKkKIEoBgb14CUIf', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'answer_query_qa', 'args': '{\"query\":\"hydration error in NEXTJS when animating dropdown menu\"}', 'id': 'call_VUUTA6T5xKkKIEoBgb14CUIf', 'index': 0, 'type': 'tool_call_chunk'}])]},\n",
       " {'steps': [AgentStep(action=ToolAgentAction(tool='answer_query_qa', tool_input={'query': 'hydration error in NEXTJS when animating dropdown menu'}, log=\"\\nInvoking: `answer_query_qa` with `{'query': 'hydration error in NEXTJS when animating dropdown menu'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_VUUTA6T5xKkKIEoBgb14CUIf', 'function': {'arguments': '{\"query\":\"hydration error in NEXTJS when animating dropdown menu\"}', 'name': 'answer_query_qa'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4'}, id='run-156dffa9-3b9c-4367-8408-d433913034c0', tool_calls=[{'name': 'answer_query_qa', 'args': {'query': 'hydration error in NEXTJS when animating dropdown menu'}, 'id': 'call_VUUTA6T5xKkKIEoBgb14CUIf', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'answer_query_qa', 'args': '{\"query\":\"hydration error in NEXTJS when animating dropdown menu\"}', 'id': 'call_VUUTA6T5xKkKIEoBgb14CUIf', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_VUUTA6T5xKkKIEoBgb14CUIf'), observation=\"To address the hydration error you're experiencing in your Next.js project while animating a dropdown menu, you can follow these steps based on a similar issue resolved in a previous discussion:\\n\\n1. **Restart the Server**: Sometimes, simply restarting the Next.js server can resolve hydration issues.\\n\\n2. **Inspect Styles**: Use the browser's inspect element tool to check if Tailwind CSS (or any other CSS framework you're using) is being loaded correctly. If the styles are present but not applied, it might indicate a configuration issue.\\n\\n3. **Verify Tailwind CSS Setup**: Ensure that Tailwind CSS is set up correctly in your project. This involves checking your Tailwind configuration file to make sure the necessary classes are being generated and applied. Refer to the Tailwind CSS documentation for detailed setup instructions.\\n\\nFor more detailed insights, you can explore the conversation with the ID `9cef8bde-c6a7-4758-a736-7d2097c0b38a` to understand how a similar issue was resolved.\")],\n",
       "  'messages': [FunctionMessage(content=\"To address the hydration error you're experiencing in your Next.js project while animating a dropdown menu, you can follow these steps based on a similar issue resolved in a previous discussion:\\n\\n1. **Restart the Server**: Sometimes, simply restarting the Next.js server can resolve hydration issues.\\n\\n2. **Inspect Styles**: Use the browser's inspect element tool to check if Tailwind CSS (or any other CSS framework you're using) is being loaded correctly. If the styles are present but not applied, it might indicate a configuration issue.\\n\\n3. **Verify Tailwind CSS Setup**: Ensure that Tailwind CSS is set up correctly in your project. This involves checking your Tailwind configuration file to make sure the necessary classes are being generated and applied. Refer to the Tailwind CSS documentation for detailed setup instructions.\\n\\nFor more detailed insights, you can explore the conversation with the ID `9cef8bde-c6a7-4758-a736-7d2097c0b38a` to understand how a similar issue was resolved.\", additional_kwargs={}, response_metadata={}, name='answer_query_qa')]},\n",
       " {'output': \"To address the hydration error you're experiencing in your Next.js project while animating a dropdown menu, you can follow these steps:\\n\\n1. **Restart the Server**: Sometimes, simply restarting the Next.js server can resolve hydration issues.\\n\\n2. **Inspect Styles**: Use the browser's inspect element tool to check if Tailwind CSS (or any other CSS framework you're using) is being loaded correctly. If the styles are present but not applied, it might indicate a configuration issue.\\n\\n3. **Verify Tailwind CSS Setup**: Ensure that Tailwind CSS is set up correctly in your project. This involves checking your Tailwind configuration file to make sure the necessary classes are being generated and applied. Refer to the Tailwind CSS documentation for detailed setup instructions.\\n\\nFor more detailed insights, you can explore the conversation with the ID `9cef8bde-c6a7-4758-a736-7d2097c0b38a` to understand how a similar issue was resolved.\",\n",
       "  'messages': [AIMessage(content=\"To address the hydration error you're experiencing in your Next.js project while animating a dropdown menu, you can follow these steps:\\n\\n1. **Restart the Server**: Sometimes, simply restarting the Next.js server can resolve hydration issues.\\n\\n2. **Inspect Styles**: Use the browser's inspect element tool to check if Tailwind CSS (or any other CSS framework you're using) is being loaded correctly. If the styles are present but not applied, it might indicate a configuration issue.\\n\\n3. **Verify Tailwind CSS Setup**: Ensure that Tailwind CSS is set up correctly in your project. This involves checking your Tailwind configuration file to make sure the necessary classes are being generated and applied. Refer to the Tailwind CSS documentation for detailed setup instructions.\\n\\nFor more detailed insights, you can explore the conversation with the ID `9cef8bde-c6a7-4758-a736-7d2097c0b38a` to understand how a similar issue was resolved.\", additional_kwargs={}, response_metadata={})]}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_main_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an assistant who can handle queries by using the tool you have and refer the user to correct question answer pairs, you just have to pass the exact user query to the tool and it will do the rest, and just give them the answer as it has come from the tool. If the tool does not give relevant answers, just tell the user that you do not know by giving the response as 'I do not know' ...\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_llm = ChatOpenAI(\n",
    "    api_key = OPENAI_API_KEY,\n",
    "    model = \"gpt-4o\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "qa_tools = [answer_query_qa]\n",
    "qa_llm_with_tools = qa_llm.bind_tools(qa_tools)\n",
    "\n",
    "\n",
    "qa_knowledge_base_agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | qa_main_prompt\n",
    "    | qa_llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")\n",
    "\n",
    "qa_agent_executor = AgentExecutor(agent=qa_knowledge_base_agent, tools=qa_tools, verbose=True)\n",
    "list(qa_agent_executor.stream({\"input\": \"I was having issues in my NEXTJS project, as I tried to animate the dropdown menu, I got a hydration error.\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FallBack Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_assistant_id = os.getenv(\"FALLBACK_OPENAI_ASSISTANT_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_thread = openAI_client.beta.threads.create()\n",
    "\n",
    "def fetch_answer_fb(final_prompt):\n",
    "    openAI_client.beta.threads.messages.create(\n",
    "        thread_id=fb_thread.id,\n",
    "        role=\"user\",\n",
    "        content= [{\n",
    "            \"type\" : \"text\",\n",
    "            \"text\" : final_prompt\n",
    "        }]\n",
    "    )\n",
    "            \n",
    "    run = openAI_client.beta.threads.runs.create(\n",
    "        thread_id=fb_thread.id,\n",
    "        assistant_id=fb_assistant_id,\n",
    "    )\n",
    "    \n",
    "    while True:\n",
    "        run = openAI_client.beta.threads.runs.retrieve(\n",
    "            thread_id=fb_thread.id,\n",
    "            run_id=run.id\n",
    "        )\n",
    "        if run.status == 'completed':\n",
    "            messages = openAI_client.beta.threads.messages.list(\n",
    "                thread_id=fb_thread.id\n",
    "            )\n",
    "            return messages.data[0].content[0].text.value\n",
    "        elif run.status in ['failed', 'cancelled', 'expired']:\n",
    "            raise Exception(f\"Run ended with status: {run.status}\")\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def answer_query_fb(query):\n",
    "    \"\"\"\n",
    "        Find relevant documents for a given query and userID.\n",
    "        \n",
    "        Args:\n",
    "        - query: The search query\n",
    "        - Admin ID : This is the id of the admin who owns the course/group\n",
    "        - Course ID : The course id of from which this conversation is from \n",
    "        - Topic ID : This will be a fixed list of id's to get easy access and for the TA's to see which topics have the most number of conversations or dounts\n",
    "        \n",
    "        Returns:\n",
    "        Final text answer from the LLM, given the relevant documents.\n",
    "    \"\"\"\n",
    "        \n",
    "    return fetch_answer_fb(query)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new None chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m[AI GENERATED]\n",
      "\n",
      "Sure! Next.js authentication can seem complex at first, but it's quite manageable once you understand the basics. Here's a high-level overview of how authentication typically works in a Next.js application:\n",
      "\n",
      "1. **Authentication Libraries**: Next.js doesn't come with a built-in authentication system, but you can use libraries like NextAuth.js or Auth0 to handle authentication. These libraries provide a set of tools and components to manage user authentication and sessions.\n",
      "\n",
      "2. **Session Management**: When a user logs in, the authentication library creates a session. This session is usually stored in a cookie or local storage. The session contains information about the user and their authentication state.\n",
      "\n",
      "3. **API Routes**: Next.js provides API routes that can be used to handle authentication requests. For example, you can create routes for login, logout, and registration. These routes can interact with your authentication library to manage user sessions.\n",
      "\n",
      "4. **Client-Side Authentication**: On the client side, you can use hooks or context provided by the authentication library to access the user's authentication state. This allows you to conditionally render components based on whether the user is logged in or not.\n",
      "\n",
      "5. **Server-Side Rendering (SSR)**: Next.js supports server-side rendering, which means you can fetch user data on the server before rendering a page. This is useful for protecting pages that require authentication. You can redirect users to a login page if they're not authenticated.\n",
      "\n",
      "6. **Protected Routes**: You can create protected routes by checking the user's authentication state before rendering a page. If the user is not authenticated, you can redirect them to a login page or show an error message.\n",
      "\n",
      "7. **Environment Variables**: Sensitive information like API keys and secret tokens should be stored in environment variables. Next.js provides a way to manage these variables securely.\n",
      "\n",
      "If you need more specific guidance or examples, I can look up relevant documentation or tutorials for you. Just let me know!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'output': \"[AI GENERATED]\\n\\nSure! Next.js authentication can seem complex at first, but it's quite manageable once you understand the basics. Here's a high-level overview of how authentication typically works in a Next.js application:\\n\\n1. **Authentication Libraries**: Next.js doesn't come with a built-in authentication system, but you can use libraries like NextAuth.js or Auth0 to handle authentication. These libraries provide a set of tools and components to manage user authentication and sessions.\\n\\n2. **Session Management**: When a user logs in, the authentication library creates a session. This session is usually stored in a cookie or local storage. The session contains information about the user and their authentication state.\\n\\n3. **API Routes**: Next.js provides API routes that can be used to handle authentication requests. For example, you can create routes for login, logout, and registration. These routes can interact with your authentication library to manage user sessions.\\n\\n4. **Client-Side Authentication**: On the client side, you can use hooks or context provided by the authentication library to access the user's authentication state. This allows you to conditionally render components based on whether the user is logged in or not.\\n\\n5. **Server-Side Rendering (SSR)**: Next.js supports server-side rendering, which means you can fetch user data on the server before rendering a page. This is useful for protecting pages that require authentication. You can redirect users to a login page if they're not authenticated.\\n\\n6. **Protected Routes**: You can create protected routes by checking the user's authentication state before rendering a page. If the user is not authenticated, you can redirect them to a login page or show an error message.\\n\\n7. **Environment Variables**: Sensitive information like API keys and secret tokens should be stored in environment variables. Next.js provides a way to manage these variables securely.\\n\\nIf you need more specific guidance or examples, I can look up relevant documentation or tutorials for you. Just let me know!\",\n",
       "  'messages': [AIMessage(content=\"[AI GENERATED]\\n\\nSure! Next.js authentication can seem complex at first, but it's quite manageable once you understand the basics. Here's a high-level overview of how authentication typically works in a Next.js application:\\n\\n1. **Authentication Libraries**: Next.js doesn't come with a built-in authentication system, but you can use libraries like NextAuth.js or Auth0 to handle authentication. These libraries provide a set of tools and components to manage user authentication and sessions.\\n\\n2. **Session Management**: When a user logs in, the authentication library creates a session. This session is usually stored in a cookie or local storage. The session contains information about the user and their authentication state.\\n\\n3. **API Routes**: Next.js provides API routes that can be used to handle authentication requests. For example, you can create routes for login, logout, and registration. These routes can interact with your authentication library to manage user sessions.\\n\\n4. **Client-Side Authentication**: On the client side, you can use hooks or context provided by the authentication library to access the user's authentication state. This allows you to conditionally render components based on whether the user is logged in or not.\\n\\n5. **Server-Side Rendering (SSR)**: Next.js supports server-side rendering, which means you can fetch user data on the server before rendering a page. This is useful for protecting pages that require authentication. You can redirect users to a login page if they're not authenticated.\\n\\n6. **Protected Routes**: You can create protected routes by checking the user's authentication state before rendering a page. If the user is not authenticated, you can redirect them to a login page or show an error message.\\n\\n7. **Environment Variables**: Sensitive information like API keys and secret tokens should be stored in environment variables. Next.js provides a way to manage these variables securely.\\n\\nIf you need more specific guidance or examples, I can look up relevant documentation or tutorials for you. Just let me know!\", additional_kwargs={}, response_metadata={})]}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb_main_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an assistant who can handle queries of students, however you are only acitvated in the case they do not understand from the main sources. Hence, YOU MUST TELL AT THE START OF YOUR ANSWER CLEARLY THAT THIS IS AI GENERATED (even a tag like [AI GENERATED] telling the user with a warning will be optimal)...\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "fb_llm = ChatOpenAI(\n",
    "    api_key = OPENAI_API_KEY,\n",
    "    model = \"gpt-4o\",\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "fb_tools = [answer_query_fb]\n",
    "fb_llm_with_tools = fb_llm.bind_tools(fb_tools)\n",
    "\n",
    "fb_agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | fb_main_prompt\n",
    "    | fb_llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")\n",
    "\n",
    "agent_executor_fb = AgentExecutor(agent=fb_agent, tools=fb_tools, verbose=True)\n",
    "list(agent_executor_fb.stream({\"input\": \"NextJS Auth seems a bit difficult...could you just tell me how it works?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph \n",
    "We will be using langgraph, which follows a graph like structure to connect multiple agents and use them as defined in a single request, along with which we can also prioritize the agents based on the importance of the agent in the request. For that, what we would be needing is the supervisor agent, which will be responsible for managing the agents and their priorities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here, we would want to have each agent as a member, which performs a specific task and then the supervisor agent, which will be responsible for managing the agents and their priorities. Ideally, the priority of the agents is currently :\n",
    "\n",
    "1. Resolver\n",
    "2. Librarian\n",
    "3. Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import MessagesState, END\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "members = [\"resolver\", \"librarian\", \"fallback\"]\n",
    "options = members + [\"FINISH\"]\n",
    "NextLiteral = Literal[\"resolver\", \"librarian\", \"fallback\", \"FINISH\"]\n",
    "\n",
    "supervisor_prompt_v1 = \"\"\"\n",
    "    You are a supervisor tasked with managing a conversation between the\n",
    "    following workers: {members}. Given the following user request,\n",
    "    respond with the worker to act next. Each worker will perform a\n",
    "    task and respond with their results and status. Ideally, you must\n",
    "    priotize the workers based on the context of the conversation.\n",
    "    If the user asks a practical or intial quetion, you should start with \"resolver\".\n",
    "    If the user aims for a more in-depth or specific answer, you should go with \"librarian\".\n",
    "    If the user asks a complex or odd question, you should use \"fallback\".\n",
    "    \n",
    "    Once you get a response from a worker, you MUST RETURN IT's OUTPUT BACK TO THE USER AND WAIT FOR THE NEXT RESPONSE.\n",
    "    REMEMBER, YOU ARE SUPPOSE TO CHOOSE THE BEST WORKER AS DEFINED ABOVE AND THEN GIVE A RESPONSE BACK TO THE USER.\n",
    "    DO NOT RUN MULTIPLE CHAINS AT ONCE, JUST ONE AT A TIME.\n",
    "    When finished, respond with FINISH.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class Router(TypedDict):\n",
    "    \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n",
    "\n",
    "    next: NextLiteral\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "\n",
    "class State(MessagesState):\n",
    "    next: str\n",
    "\n",
    "\n",
    "def supervisor_node(state: State) -> Command[Literal[\"resolver\", \"librarian\", \"fallback\", \"__end__\"]]:\n",
    "    supervisor_prompt = supervisor_prompt_v1.format(members=\", \".join(members))\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": supervisor_prompt},\n",
    "    ] + state[\"messages\"]\n",
    "    \n",
    "    response = llm.with_structured_output(Router).invoke(messages)\n",
    "    \n",
    "    goto = response[\"next\"]\n",
    "    if goto == \"FINISH\":\n",
    "        goto = END\n",
    "\n",
    "    return Command(goto=goto, update={\"next\": goto})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worker Agent Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolver_node(state: State) :\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "    agent_input = {\"input\": user_message}\n",
    "    \n",
    "    result = qa_agent_executor.invoke(agent_input)\n",
    "    output_answer = result[\"output\"]\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=output_answer, name=\"resolver\"\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "def librarian_node(state: State) :\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "    agent_input = {\"input\": user_message}\n",
    "    \n",
    "    result = text_knowledge_base_agent_agent_executor.invoke(agent_input)\n",
    "    output_answer = result[\"output\"]\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=output_answer, name=\"resolver\"\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "def fallback_node(state: State) :\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "    agent_input = {\"input\": user_message}\n",
    "    \n",
    "    result = agent_executor_fb.invoke(agent_input)\n",
    "    output_answer = result[\"output\"]\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=output_answer, name=\"resolver\"\n",
    "            )\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x153983910>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add nodes and edges\n",
    "workflow.add_edge(START, \"supervisor\")\n",
    "workflow.add_node(\"supervisor\", supervisor_node)\n",
    "workflow.add_node(\"resolver\", resolver_node)\n",
    "workflow.add_node(\"librarian\", librarian_node)\n",
    "workflow.add_node(\"fallback\", fallback_node)\n",
    "\n",
    "workflow.add_edge(\"resolver\",\"supervisor\")\n",
    "workflow.add_edge(\"librarian\",\"supervisor\")\n",
    "workflow.add_edge(\"fallback\",\"supervisor\")\n",
    "\n",
    "workflow.add_edge(\"supervisor\", END)\n",
    "\n",
    "graph = workflow.compile()\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating the Agent Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Pregel.stream at 0x12ffaa700>\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `answer_query_qa` with `{'query': 'NextJS hydration error'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mIf you're dealing with a hydration error in your Next.js project, particularly when animating a dropdown menu, here's a solution based on a similar issue that was resolved previously:\n",
      "\n",
      "1. **Restart the Server**: Restarting the Next.js server can sometimes resolve hydration issues.\n",
      "\n",
      "2. **Inspect Styles**: Use the browser's inspect element tool to ensure that Tailwind CSS (or any other CSS framework you're using) is being loaded correctly. If the styles are present but not applied, it might indicate a configuration issue.\n",
      "\n",
      "3. **Verify Tailwind CSS Setup**: Make sure that Tailwind CSS is set up correctly in your project. This involves checking your Tailwind configuration file to ensure that the necessary classes are being generated and applied. Refer to the Tailwind CSS documentation for detailed setup instructions.\n",
      "\n",
      "For more detailed insights, you can explore the conversation with the ID `9cef8bde-c6a7-4758-a736-7d2097c0b38a` to understand how a similar issue was resolved. This thread might provide additional context and solutions that could be applicable to your situation.\u001b[0m\u001b[32;1m\u001b[1;3mIf you're dealing with a hydration error in your Next.js project, particularly when animating a dropdown menu, here's a solution based on a similar issue that was resolved previously:\n",
      "\n",
      "1. **Restart the Server**: Restarting the Next.js server can sometimes resolve hydration issues.\n",
      "\n",
      "2. **Inspect Styles**: Use the browser's inspect element tool to ensure that Tailwind CSS (or any other CSS framework you're using) is being loaded correctly. If the styles are present but not applied, it might indicate a configuration issue.\n",
      "\n",
      "3. **Verify Tailwind CSS Setup**: Make sure that Tailwind CSS is set up correctly in your project. This involves checking your Tailwind configuration file to ensure that the necessary classes are being generated and applied. Refer to the Tailwind CSS documentation for detailed setup instructions.\n",
      "\n",
      "For more detailed insights, you can explore the conversation with the ID `9cef8bde-c6a7-4758-a736-7d2097c0b38a` to understand how a similar issue was resolved. This thread might provide additional context and solutions that could be applicable to your situation.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `answer_query_qa` with `{'query': 'hydration error in Next.js project when animating a dropdown menu'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mTo address the hydration error you're experiencing in your Next.js project when animating a dropdown menu, you can follow these steps based on a similar issue resolved previously:\n",
      "\n",
      "1. **Restart the Server**: Restarting the Next.js server can sometimes resolve hydration issues.\n",
      "\n",
      "2. **Inspect Styles**: Use the browser's inspect element tool to ensure that Tailwind CSS (or any other CSS framework you're using) is being loaded correctly. If the styles are present but not applied, it might indicate a configuration issue.\n",
      "\n",
      "3. **Verify Tailwind CSS Setup**: Make sure that Tailwind CSS is set up correctly in your project. This involves checking your Tailwind configuration file to ensure that the necessary classes are being generated and applied. Refer to the Tailwind CSS documentation for detailed setup instructions.\n",
      "\n",
      "For more detailed insights, you can explore the conversation with the ID `9cef8bde-c6a7-4758-a736-7d2097c0b38a` to understand how a similar issue was resolved. This thread might provide additional context and solutions that could be applicable to your situation.\u001b[0m\u001b[32;1m\u001b[1;3mTo address the hydration error you're experiencing in your Next.js project when animating a dropdown menu, you can follow these steps based on a similar issue resolved previously:\n",
      "\n",
      "1. **Restart the Server**: Restarting the Next.js server can sometimes resolve hydration issues.\n",
      "\n",
      "2. **Inspect Styles**: Use the browser's inspect element tool to ensure that Tailwind CSS (or any other CSS framework you're using) is being loaded correctly. If the styles are present but not applied, it might indicate a configuration issue.\n",
      "\n",
      "3. **Verify Tailwind CSS Setup**: Make sure that Tailwind CSS is set up correctly in your project. This involves checking your Tailwind configuration file to ensure that the necessary classes are being generated and applied. Refer to the Tailwind CSS documentation for detailed setup instructions.\n",
      "\n",
      "For more detailed insights, you can explore the conversation with the ID `9cef8bde-c6a7-4758-a736-7d2097c0b38a` to understand how a similar issue was resolved. This thread might provide additional context and solutions that could be applicable to your situation.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `kb_answer_query` with `{'query': 'conversation with the ID 9cef8bde-c6a7-4758-a736-7d2097c0b38a'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mThe documents provided do not contain any information related to a conversation with the ID \"9cef8bde-c6a7-4758-a736-7d2097c0b38a\". It seems that this specific conversation ID is not referenced in the available documents. If you have access to other resources or databases, you might want to check there for more information regarding this conversation ID.\u001b[0m\u001b[32;1m\u001b[1;3mI do not know.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI do not know.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI do not know.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI do not know.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Update state based on the graph response\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m:\n\u001b[1;32m     31\u001b[0m     initial_state\u001b[38;5;241m.\u001b[39mupdate(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Check if the conversation has finished\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1670\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1664\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1665\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1667\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1670\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   1677\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langgraph/pregel/runner.py:230\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    228\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langgraph/utils/runnable.py:462\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    458\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m    459\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    460\u001b[0m )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 462\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langgraph/utils/runnable.py:226\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 226\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[114], line 53\u001b[0m, in \u001b[0;36msupervisor_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     48\u001b[0m supervisor_prompt \u001b[38;5;241m=\u001b[39m supervisor_prompt_v1\u001b[38;5;241m.\u001b[39mformat(members\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(members))\n\u001b[1;32m     49\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     50\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: supervisor_prompt},\n\u001b[1;32m     51\u001b[0m ] \u001b[38;5;241m+\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 53\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_structured_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRouter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m goto \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m goto \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFINISH\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/runnables/base.py:3020\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3018\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m   3019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3020\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3021\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3022\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/runnables/base.py:5352\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5347\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5348\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   5349\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5350\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5351\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5353\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5354\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5356\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:790\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    784\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    788\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    789\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:647\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    646\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 647\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    648\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    649\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    651\u001b[0m ]\n\u001b[1;32m    652\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:637\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    636\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 637\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m         )\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    645\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:855\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:782\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    780\u001b[0m payload\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 782\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mBadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    784\u001b[0m     _handle_openai_bad_request(e)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/resources/beta/chat/completions.py:160\u001b[0m, in \u001b[0;36mCompletions.parse\u001b[0;34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparser\u001b[39m(raw_completion: ChatCompletion) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ParsedChatCompletion[ResponseFormatT]:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[1;32m    155\u001b[0m         response_format\u001b[38;5;241m=\u001b[39mresponse_format,\n\u001b[1;32m    156\u001b[0m         chat_completion\u001b[38;5;241m=\u001b[39mraw_completion,\n\u001b[1;32m    157\u001b[0m         input_tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    158\u001b[0m     )\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;49;00m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedChatCompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1282\u001b[0m     )\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:996\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    993\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 996\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1002\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpx/_client.py:915\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    907\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    911\u001b[0m )\n\u001b[1;32m    913\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 915\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpx/_client.py:943\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpx/_client.py:980\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    978\u001b[0m     hook(request)\n\u001b[0;32m--> 980\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpx/_client.py:1016\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1012\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1013\u001b[0m     )\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1016\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1020\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpx/_transports/default.py:231\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    218\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    219\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    220\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    229\u001b[0m )\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 231\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    236\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    237\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    238\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    239\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    240\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_closed(status)\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# up as HTTP/1.1.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool_lock:\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;66;03m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;66;03m# status so that the request becomes queued again.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpcore/_sync/http11.py:133\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpcore/_sync/http11.py:111\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    105\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    106\u001b[0m     (\n\u001b[1;32m    107\u001b[0m         http_version,\n\u001b[1;32m    108\u001b[0m         status,\n\u001b[1;32m    109\u001b[0m         reason_phrase,\n\u001b[1;32m    110\u001b[0m         headers,\n\u001b[0;32m--> 111\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    113\u001b[0m         http_version,\n\u001b[1;32m    114\u001b[0m         status,\n\u001b[1;32m    115\u001b[0m         reason_phrase,\n\u001b[1;32m    116\u001b[0m         headers,\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    120\u001b[0m     status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m    121\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     },\n\u001b[1;32m    128\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpcore/_sync/http11.py:176\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    173\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpcore/_sync/http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 212\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1263\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1260\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1261\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1262\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1136\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_state = State(messages=[],next=\"supervisor\")\n",
    "while True:\n",
    "    # Display current conversation history\n",
    "    if \"messages\" in initial_state and initial_state[\"messages\"]:\n",
    "        for msg in initial_state[\"messages\"]:\n",
    "            print(f\"{msg['role'].capitalize()}: {msg['content']}\")\n",
    "\n",
    "    # Get user input\n",
    "    user_input = input(\"You: \")\n",
    "    \n",
    "    # Check for exit condition\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Exiting the conversation. Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    # Append the user input to the state\n",
    "    initial_state[\"messages\"].append({\"role\": \"user\", \"content\": user_input})\n",
    "    initial_state[\"input\"] = user_input\n",
    "\n",
    "    # Invoke the graph to process the user input\n",
    "    try:\n",
    "        # result = graph.invoke(initial_state)\n",
    "        result = graph.stream(initial_state)\n",
    "        print(result)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        break\n",
    "    \n",
    "    # Update state based on the graph response\n",
    "    if \"update\" in result:\n",
    "        initial_state.update(result[\"update\"])\n",
    "    \n",
    "    # Check if the conversation has finished\n",
    "    if result.get(\"goto\") == \"__end__\":\n",
    "        print(\"Supervisor: FINISH\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'user',\n",
       "   'content': 'I am having issues with NextJS hydration error'}],\n",
       " 'next': 'supervisor',\n",
       " 'input': 'I am having issues with NextJS hydration error'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
