{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader,TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from openai import OpenAI\n",
    "import requests\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "    format_to_openai_tool_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import List, Union, Dict, Optional\n",
    "from pinecone import Index\n",
    "\n",
    "import validators\n",
    "import requests\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_API_ENV = os.getenv(\"PINECONE_API_ENV\")\n",
    "PINECONE_KNOWLEDGE_BASE_INDEX_NAME = os.getenv(\"PINECONE_KNOWLEDGE_BASE_INDEX_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client Initialisation\n",
    "Now, we will be using the openAI client to get embeddings, as we would be needing in our model along with neccessary intialisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openAI_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "embedding_model = openAI_client.embeddings\n",
    "pc = Pinecone(api_key = PINECONE_API_KEY, environment = PINECONE_API_ENV)\n",
    "index = pc.Index(PINECONE_KNOWLEDGE_BASE_INDEX_NAME)\n",
    "\n",
    "def get_embedding(text) :\n",
    "    \"\"\"\n",
    "        Function to convert the text string into embeddings using text-embedding-3-small from OpenAI\n",
    "    \n",
    "        Args:\n",
    "            text : A string which will contain either the text chunk or the user query\n",
    "            \n",
    "        Returns:\n",
    "            vector : A vector of 1536 dimensions\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = embedding_model.create(\n",
    "            input=text,\n",
    "            model=\"text-embedding-3-small\"\n",
    "        )\n",
    "        \n",
    "        return response.data[0].embedding   \n",
    "    \n",
    "    except Exception as e:\n",
    "        raise Exception(str(e))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to use the above, we need to create these functions as a collective tool, which can be used by an agent to query the vector DB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool Creation\n",
    "Now, we would have to create tools for this purpose, as asynchronus functions which await till the vector DB is loaded and then query the vector DB, evnetually returning the relevant texts. As the admin id will be a fixed one, we will be directly referring it from the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_chunks(query: str, userID: str):\n",
    "    \"\"\"\n",
    "    Find relevant documents for a given query and userID.\n",
    "    \n",
    "    Args:\n",
    "    - query: The search query\n",
    "    - userID: The ID of the user\n",
    "    \n",
    "    Returns:\n",
    "    List of relevant document chunks\n",
    "    \"\"\"\n",
    "    query_vector = get_embedding(query)\n",
    "    results = index.query(\n",
    "        vector=query_vector,\n",
    "        top_k=5,\n",
    "        include_values=False,\n",
    "        include_metadata=True,\n",
    "        filter={\n",
    "            \"userID\": userID\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    relevant_texts = []\n",
    "    for record in results['matches']:\n",
    "        text = {\n",
    "            'score': record['score'],\n",
    "            'text': record['metadata']['chunk'],\n",
    "            'name': record['metadata']['document_name'],\n",
    "            'reference': int(record[\"metadata\"][\"page_number\"]) + 1\n",
    "        }\n",
    "        relevant_texts.append(text)\n",
    "    \n",
    "    return relevant_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the answer generation, let us now set the system instructions for the LLM model, which would be used to generate the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instructions = \"\"\"\n",
    "    You are a specialised AI document analyser working at an edtech startup, and you will be assisting the users to answer their queries. You will be given \n",
    "    the top relevant documents and you have to use those to answer the query asked by the user, which will be given to you below. \n",
    "    \n",
    "    In the relevant documents,you will be given the cosine similarity score, the document name and the reference (which is the page number where this \n",
    "    text was in the document) and the text itself. You can in you answer integrate the document name and reference to build authenticity of your answer, \n",
    "    by precisely writing it like (reference page : page_num). You should eventually tell the user to explore more about the topic in the document and that \n",
    "    reference page. Infact, this would be highly favourable if you mention the document name and reference to build your authenticity\n",
    "    \n",
    "    MAKE SURE YOU DO NOT ANSWER FROM ANYTHING APART FROM THE DOCUMENTS GIVEN TO YOU. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would also need the query prompt created from a prompt template, by which we can dynamically populate the query prompt with the relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prompt_template = \"\"\"\n",
    "    \\n\\n User Query : {query}\n",
    "    \\n\\n Documents : {documents}\n",
    "\"\"\"\n",
    "\n",
    "query_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\",\"documents\"],\n",
    "    template= query_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assistant API \n",
    "Now, we have an assistant ready to be used, which can be used to go through the query and relevant documents, and then generate the answers. We will be using hitting the assistant with an API call, and then getting the answers from the assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "openAI_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "assistant_id = os.getenv(\"KNOWLEDGE_BASE_OPENAI_ASSISTANT_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will be building the function to answer the queries from the assistant, which would be an asynchronus function on a thread as we aim to fetch the answer from the assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = openAI_client.beta.threads.create()\n",
    "\n",
    "def fetch_answer(final_prompt):\n",
    "    openAI_client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content= [{\n",
    "            \"type\" : \"text\",\n",
    "            \"text\" : final_prompt\n",
    "        }]\n",
    "    )\n",
    "            \n",
    "    run = openAI_client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant_id,\n",
    "    )\n",
    "    \n",
    "    while True:\n",
    "        run = openAI_client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id\n",
    "        )\n",
    "        if run.status == 'completed':\n",
    "            messages = openAI_client.beta.threads.messages.list(\n",
    "                thread_id=thread.id\n",
    "            )\n",
    "            return messages.data[0].content[0].text.value\n",
    "        elif run.status in ['failed', 'cancelled', 'expired']:\n",
    "            raise Exception(f\"Run ended with status: {run.status}\")\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "Now, we will have to make the tools for the agent and make it ready to be used. We will be using the agent to fetch the user query (ideally, this would be coming in from the supervisor), and then we will be using the agent to fetch the answers from the assistant (eventually to be sent back to the supervisor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def answer_query(query):\n",
    "    \"\"\"\n",
    "        Find relevant documents for a given query and userID.\n",
    "        \n",
    "        Args:\n",
    "        - query: The search query\n",
    "        \n",
    "        Returns:\n",
    "        Final text answer from the LLM, given the relevant documents.\n",
    "    \"\"\"\n",
    "    query_vector = get_embedding(query)\n",
    "    results = index.query(\n",
    "        vector=query_vector,\n",
    "        top_k=10,\n",
    "        include_values=False,\n",
    "        include_metadata=True,\n",
    "        filter={\n",
    "            \"userID\": os.getenv(\"ADMIN_ID\")\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    relevant_texts = []\n",
    "    for record in results['matches']:\n",
    "        text = {\n",
    "            'score': record['score'],\n",
    "            'text': record['metadata']['chunk'],\n",
    "            'name': record['metadata']['document_name'],\n",
    "            'reference': int(record[\"metadata\"][\"page_number\"]) + 1\n",
    "        }\n",
    "        relevant_texts.append(text)\n",
    "    \n",
    "    final_prompt = query_prompt.invoke({\"query\" : query,\"documents\": relevant_texts}).text\n",
    "    return fetch_answer(final_prompt)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the tool ready, we will be making the prompt for the agent. This aims to clearly tell the assisant what has to be done and what is the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an assistant who can handle queries by using the tool you have and refer the user to correct material, you just have to pass the exact user query to the tool and it will do the rest, you DO NOT HAVE TO GIVE ANY USERID TO ANY TOOL, they have the id pre-defined...\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now initialise the LLM instance from langchain and then use the tools above and bind them to the agent, which would be used to fetch the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    api_key = OPENAI_API_KEY,\n",
    "    model = \"gpt-4o\",\n",
    "    temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [answer_query]\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this function creates an agent pipeline in a step-by-step flow, processing input data and returning a meaningful output. How the Pipeline Works Together : \n",
    "- Input Preparation: The user’s query (x[\"input\"]) and intermediate steps are extracted and formatted.\n",
    "- Main Prompt Application: The instructions for the agent (e.g., \"You are a helpful assistant\") are added to guide its behavior.\n",
    "- Processing by LLM: The LLM processes the input and, if needed, interacts with tools to generate an appropriate response.\n",
    "- Output Parsing: The raw response from the LLM is cleaned up for easy understanding by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | main_prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can execute the function and see how the agent works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new None chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `answer_query` with `{'query': 'what is meant by rolling up the state?'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\"Rolling up the state\" refers to a technique in React where you manage state more efficiently by lifting it up to the least common ancestor (LCA) of components that need access to that state. Instead of duplicating the state across multiple components, you centralize it in a common ancestor, allowing that ancestor to manage the state and pass it down to its children as needed. This approach helps in reducing unoptimal re-renders and makes the state management more streamlined as your application grows (W9-react, reference page: 37).\n",
      "\n",
      "For more details on this concept, you can explore the document \"W9-react\" on reference page 37.\u001b[0m\u001b[32;1m\u001b[1;3m\"Rolling up the state\" in React refers to managing state more efficiently by lifting it up to the least common ancestor (LCA) of components that need access to that state. Instead of duplicating the state across multiple components, you centralize it in a common ancestor, allowing that ancestor to manage the state and pass it down to its children as needed. This approach helps reduce unoptimal re-renders and streamlines state management as your application grows. For more details, you can explore the document \"W9-react\" on reference page 37.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'actions': [ToolAgentAction(tool='answer_query', tool_input={'query': 'what is meant by rolling up the state?'}, log=\"\\nInvoking: `answer_query` with `{'query': 'what is meant by rolling up the state?'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_LO5pigznnR2OX1i5ZWujkC3j', 'function': {'arguments': '{\"query\":\"what is meant by rolling up the state?\"}', 'name': 'answer_query'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4'}, id='run-2be70ff1-84c3-4ed7-af12-e5d1f8bffccf', tool_calls=[{'name': 'answer_query', 'args': {'query': 'what is meant by rolling up the state?'}, 'id': 'call_LO5pigznnR2OX1i5ZWujkC3j', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'answer_query', 'args': '{\"query\":\"what is meant by rolling up the state?\"}', 'id': 'call_LO5pigznnR2OX1i5ZWujkC3j', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_LO5pigznnR2OX1i5ZWujkC3j')],\n",
       "  'messages': [AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_LO5pigznnR2OX1i5ZWujkC3j', 'function': {'arguments': '{\"query\":\"what is meant by rolling up the state?\"}', 'name': 'answer_query'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4'}, id='run-2be70ff1-84c3-4ed7-af12-e5d1f8bffccf', tool_calls=[{'name': 'answer_query', 'args': {'query': 'what is meant by rolling up the state?'}, 'id': 'call_LO5pigznnR2OX1i5ZWujkC3j', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'answer_query', 'args': '{\"query\":\"what is meant by rolling up the state?\"}', 'id': 'call_LO5pigznnR2OX1i5ZWujkC3j', 'index': 0, 'type': 'tool_call_chunk'}])]},\n",
       " {'steps': [AgentStep(action=ToolAgentAction(tool='answer_query', tool_input={'query': 'what is meant by rolling up the state?'}, log=\"\\nInvoking: `answer_query` with `{'query': 'what is meant by rolling up the state?'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_LO5pigznnR2OX1i5ZWujkC3j', 'function': {'arguments': '{\"query\":\"what is meant by rolling up the state?\"}', 'name': 'answer_query'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4'}, id='run-2be70ff1-84c3-4ed7-af12-e5d1f8bffccf', tool_calls=[{'name': 'answer_query', 'args': {'query': 'what is meant by rolling up the state?'}, 'id': 'call_LO5pigznnR2OX1i5ZWujkC3j', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'answer_query', 'args': '{\"query\":\"what is meant by rolling up the state?\"}', 'id': 'call_LO5pigznnR2OX1i5ZWujkC3j', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_LO5pigznnR2OX1i5ZWujkC3j'), observation='\"Rolling up the state\" refers to a technique in React where you manage state more efficiently by lifting it up to the least common ancestor (LCA) of components that need access to that state. Instead of duplicating the state across multiple components, you centralize it in a common ancestor, allowing that ancestor to manage the state and pass it down to its children as needed. This approach helps in reducing unoptimal re-renders and makes the state management more streamlined as your application grows (W9-react, reference page: 37).\\n\\nFor more details on this concept, you can explore the document \"W9-react\" on reference page 37.')],\n",
       "  'messages': [FunctionMessage(content='\"Rolling up the state\" refers to a technique in React where you manage state more efficiently by lifting it up to the least common ancestor (LCA) of components that need access to that state. Instead of duplicating the state across multiple components, you centralize it in a common ancestor, allowing that ancestor to manage the state and pass it down to its children as needed. This approach helps in reducing unoptimal re-renders and makes the state management more streamlined as your application grows (W9-react, reference page: 37).\\n\\nFor more details on this concept, you can explore the document \"W9-react\" on reference page 37.', additional_kwargs={}, response_metadata={}, name='answer_query')]},\n",
       " {'output': '\"Rolling up the state\" in React refers to managing state more efficiently by lifting it up to the least common ancestor (LCA) of components that need access to that state. Instead of duplicating the state across multiple components, you centralize it in a common ancestor, allowing that ancestor to manage the state and pass it down to its children as needed. This approach helps reduce unoptimal re-renders and streamlines state management as your application grows. For more details, you can explore the document \"W9-react\" on reference page 37.',\n",
       "  'messages': [AIMessage(content='\"Rolling up the state\" in React refers to managing state more efficiently by lifting it up to the least common ancestor (LCA) of components that need access to that state. Instead of duplicating the state across multiple components, you centralize it in a common ancestor, allowing that ancestor to manage the state and pass it down to its children as needed. This approach helps reduce unoptimal re-renders and streamlines state management as your application grows. For more details, you can explore the document \"W9-react\" on reference page 37.', additional_kwargs={}, response_metadata={})]}]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "list(agent_executor.stream({\"input\": \"what is meant by rolling up the state?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new None chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `answer_query` with `{'query': 'useRef in React'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mIn React, `useRef` is a hook that allows you to create a reference to a value or a DOM element that persists across renders. One of the key characteristics of `useRef` is that it does not trigger a re-render when the value it holds changes. This makes it particularly useful for storing mutable values that you want to persist without causing additional renders (W9-react, reference page: 25).\n",
      "\n",
      "A common use case for `useRef` is to access and manipulate DOM elements directly. For example, you can create a ref to an input element and use it to focus the input programmatically (W9-react, reference page: 26).\n",
      "\n",
      "To explore more about `useRef` and see examples of its usage, you can refer to the document \"W9-react\" on reference pages 25 and 26.\u001b[0m\u001b[32;1m\u001b[1;3mIn React, `useRef` is a hook that allows you to create a reference to a value or a DOM element that persists across renders. It doesn't trigger a re-render when the value it holds changes, making it useful for storing mutable values without causing additional renders.\n",
      "\n",
      "Commonly, `useRef` is used to access and manipulate DOM elements directly, such as focusing an input programmatically.\n",
      "\n",
      "For more details and examples, you can refer to the document \"W9-react\" on reference pages 25 and 26.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'actions': [ToolAgentAction(tool='answer_query', tool_input={'query': 'useRef in React'}, log=\"\\nInvoking: `answer_query` with `{'query': 'useRef in React'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_8pAykBBbRK3ip0vP0lzSyFqh', 'function': {'arguments': '{\"query\":\"useRef in React\"}', 'name': 'answer_query'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4'}, id='run-112e8f95-dfe9-4c60-84a2-928e743e65f1', tool_calls=[{'name': 'answer_query', 'args': {'query': 'useRef in React'}, 'id': 'call_8pAykBBbRK3ip0vP0lzSyFqh', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'answer_query', 'args': '{\"query\":\"useRef in React\"}', 'id': 'call_8pAykBBbRK3ip0vP0lzSyFqh', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_8pAykBBbRK3ip0vP0lzSyFqh')],\n",
       "  'messages': [AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_8pAykBBbRK3ip0vP0lzSyFqh', 'function': {'arguments': '{\"query\":\"useRef in React\"}', 'name': 'answer_query'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4'}, id='run-112e8f95-dfe9-4c60-84a2-928e743e65f1', tool_calls=[{'name': 'answer_query', 'args': {'query': 'useRef in React'}, 'id': 'call_8pAykBBbRK3ip0vP0lzSyFqh', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'answer_query', 'args': '{\"query\":\"useRef in React\"}', 'id': 'call_8pAykBBbRK3ip0vP0lzSyFqh', 'index': 0, 'type': 'tool_call_chunk'}])]},\n",
       " {'steps': [AgentStep(action=ToolAgentAction(tool='answer_query', tool_input={'query': 'useRef in React'}, log=\"\\nInvoking: `answer_query` with `{'query': 'useRef in React'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_8pAykBBbRK3ip0vP0lzSyFqh', 'function': {'arguments': '{\"query\":\"useRef in React\"}', 'name': 'answer_query'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4'}, id='run-112e8f95-dfe9-4c60-84a2-928e743e65f1', tool_calls=[{'name': 'answer_query', 'args': {'query': 'useRef in React'}, 'id': 'call_8pAykBBbRK3ip0vP0lzSyFqh', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'answer_query', 'args': '{\"query\":\"useRef in React\"}', 'id': 'call_8pAykBBbRK3ip0vP0lzSyFqh', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_8pAykBBbRK3ip0vP0lzSyFqh'), observation='In React, `useRef` is a hook that allows you to create a reference to a value or a DOM element that persists across renders. One of the key characteristics of `useRef` is that it does not trigger a re-render when the value it holds changes. This makes it particularly useful for storing mutable values that you want to persist without causing additional renders (W9-react, reference page: 25).\\n\\nA common use case for `useRef` is to access and manipulate DOM elements directly. For example, you can create a ref to an input element and use it to focus the input programmatically (W9-react, reference page: 26).\\n\\nTo explore more about `useRef` and see examples of its usage, you can refer to the document \"W9-react\" on reference pages 25 and 26.')],\n",
       "  'messages': [FunctionMessage(content='In React, `useRef` is a hook that allows you to create a reference to a value or a DOM element that persists across renders. One of the key characteristics of `useRef` is that it does not trigger a re-render when the value it holds changes. This makes it particularly useful for storing mutable values that you want to persist without causing additional renders (W9-react, reference page: 25).\\n\\nA common use case for `useRef` is to access and manipulate DOM elements directly. For example, you can create a ref to an input element and use it to focus the input programmatically (W9-react, reference page: 26).\\n\\nTo explore more about `useRef` and see examples of its usage, you can refer to the document \"W9-react\" on reference pages 25 and 26.', additional_kwargs={}, response_metadata={}, name='answer_query')]},\n",
       " {'output': 'In React, `useRef` is a hook that allows you to create a reference to a value or a DOM element that persists across renders. It doesn\\'t trigger a re-render when the value it holds changes, making it useful for storing mutable values without causing additional renders.\\n\\nCommonly, `useRef` is used to access and manipulate DOM elements directly, such as focusing an input programmatically.\\n\\nFor more details and examples, you can refer to the document \"W9-react\" on reference pages 25 and 26.',\n",
       "  'messages': [AIMessage(content='In React, `useRef` is a hook that allows you to create a reference to a value or a DOM element that persists across renders. It doesn\\'t trigger a re-render when the value it holds changes, making it useful for storing mutable values without causing additional renders.\\n\\nCommonly, `useRef` is used to access and manipulate DOM elements directly, such as focusing an input programmatically.\\n\\nFor more details and examples, you can refer to the document \"W9-react\" on reference pages 25 and 26.', additional_kwargs={}, response_metadata={})]}]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "list(agent_executor.stream({\"input\": \"I am so confused in useRef bruh...\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new None chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `answer_query` with `{'query': 'prop drelling example'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mProp drilling in React occurs when you need to pass data from a higher-level component down to a lower-level component that is several layers deep in the component tree. This often involves passing props through many intermediate components that do not use the props themselves, just to get them to the component that needs them. This can lead to increased complexity and maintenance challenges (W9-react, reference page: 39).\n",
      "\n",
      "For example, if you have a component hierarchy like this:\n",
      "\n",
      "```\n",
      "<App>\n",
      "  <Parent>\n",
      "    <Child>\n",
      "      <Grandchild>\n",
      "        <TargetComponent />\n",
      "      </Grandchild>\n",
      "    </Child>\n",
      "  </Parent>\n",
      "</App>\n",
      "```\n",
      "\n",
      "If `TargetComponent` needs a piece of data from `App`, you would have to pass that data down through `Parent`, `Child`, and `Grandchild`, even if those components do not need to use the data themselves. This is what is referred to as prop drilling.\n",
      "\n",
      "To explore more about prop drilling and its implications, you can refer to the document \"W9-react\" on reference page 39.\u001b[0m\u001b[32;1m\u001b[1;3mProp drilling in React refers to the process of passing data from a higher-level component to a lower-level component through multiple layers of intermediate components that do not use the data themselves. This can make the code more complex and harder to maintain.\n",
      "\n",
      "**Example:**\n",
      "\n",
      "Consider a component hierarchy like this:\n",
      "\n",
      "```\n",
      "<App>\n",
      "  <Parent>\n",
      "    <Child>\n",
      "      <Grandchild>\n",
      "        <TargetComponent />\n",
      "      </Grandchild>\n",
      "    </Child>\n",
      "  </Parent>\n",
      "</App>\n",
      "```\n",
      "\n",
      "If `TargetComponent` needs a piece of data from `App`, you would have to pass that data through `Parent`, `Child`, and `Grandchild`, even though these components do not use the data themselves. This is known as prop drilling.\n",
      "\n",
      "For more details, you can refer to the document \"W9-react\" on reference page 39.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'actions': [ToolAgentAction(tool='answer_query', tool_input={'query': 'prop drelling example'}, log=\"\\nInvoking: `answer_query` with `{'query': 'prop drelling example'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_h0LzmeJF4jCIFYGftdctGnta', 'function': {'arguments': '{\"query\":\"prop drelling example\"}', 'name': 'answer_query'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4'}, id='run-1fb71265-6207-47f8-9aa6-d739d33202eb', tool_calls=[{'name': 'answer_query', 'args': {'query': 'prop drelling example'}, 'id': 'call_h0LzmeJF4jCIFYGftdctGnta', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'answer_query', 'args': '{\"query\":\"prop drelling example\"}', 'id': 'call_h0LzmeJF4jCIFYGftdctGnta', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_h0LzmeJF4jCIFYGftdctGnta')],\n",
       "  'messages': [AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_h0LzmeJF4jCIFYGftdctGnta', 'function': {'arguments': '{\"query\":\"prop drelling example\"}', 'name': 'answer_query'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4'}, id='run-1fb71265-6207-47f8-9aa6-d739d33202eb', tool_calls=[{'name': 'answer_query', 'args': {'query': 'prop drelling example'}, 'id': 'call_h0LzmeJF4jCIFYGftdctGnta', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'answer_query', 'args': '{\"query\":\"prop drelling example\"}', 'id': 'call_h0LzmeJF4jCIFYGftdctGnta', 'index': 0, 'type': 'tool_call_chunk'}])]},\n",
       " {'steps': [AgentStep(action=ToolAgentAction(tool='answer_query', tool_input={'query': 'prop drelling example'}, log=\"\\nInvoking: `answer_query` with `{'query': 'prop drelling example'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_h0LzmeJF4jCIFYGftdctGnta', 'function': {'arguments': '{\"query\":\"prop drelling example\"}', 'name': 'answer_query'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4'}, id='run-1fb71265-6207-47f8-9aa6-d739d33202eb', tool_calls=[{'name': 'answer_query', 'args': {'query': 'prop drelling example'}, 'id': 'call_h0LzmeJF4jCIFYGftdctGnta', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'answer_query', 'args': '{\"query\":\"prop drelling example\"}', 'id': 'call_h0LzmeJF4jCIFYGftdctGnta', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_h0LzmeJF4jCIFYGftdctGnta'), observation='Prop drilling in React occurs when you need to pass data from a higher-level component down to a lower-level component that is several layers deep in the component tree. This often involves passing props through many intermediate components that do not use the props themselves, just to get them to the component that needs them. This can lead to increased complexity and maintenance challenges (W9-react, reference page: 39).\\n\\nFor example, if you have a component hierarchy like this:\\n\\n```\\n<App>\\n  <Parent>\\n    <Child>\\n      <Grandchild>\\n        <TargetComponent />\\n      </Grandchild>\\n    </Child>\\n  </Parent>\\n</App>\\n```\\n\\nIf `TargetComponent` needs a piece of data from `App`, you would have to pass that data down through `Parent`, `Child`, and `Grandchild`, even if those components do not need to use the data themselves. This is what is referred to as prop drilling.\\n\\nTo explore more about prop drilling and its implications, you can refer to the document \"W9-react\" on reference page 39.')],\n",
       "  'messages': [FunctionMessage(content='Prop drilling in React occurs when you need to pass data from a higher-level component down to a lower-level component that is several layers deep in the component tree. This often involves passing props through many intermediate components that do not use the props themselves, just to get them to the component that needs them. This can lead to increased complexity and maintenance challenges (W9-react, reference page: 39).\\n\\nFor example, if you have a component hierarchy like this:\\n\\n```\\n<App>\\n  <Parent>\\n    <Child>\\n      <Grandchild>\\n        <TargetComponent />\\n      </Grandchild>\\n    </Child>\\n  </Parent>\\n</App>\\n```\\n\\nIf `TargetComponent` needs a piece of data from `App`, you would have to pass that data down through `Parent`, `Child`, and `Grandchild`, even if those components do not need to use the data themselves. This is what is referred to as prop drilling.\\n\\nTo explore more about prop drilling and its implications, you can refer to the document \"W9-react\" on reference page 39.', additional_kwargs={}, response_metadata={}, name='answer_query')]},\n",
       " {'output': 'Prop drilling in React refers to the process of passing data from a higher-level component to a lower-level component through multiple layers of intermediate components that do not use the data themselves. This can make the code more complex and harder to maintain.\\n\\n**Example:**\\n\\nConsider a component hierarchy like this:\\n\\n```\\n<App>\\n  <Parent>\\n    <Child>\\n      <Grandchild>\\n        <TargetComponent />\\n      </Grandchild>\\n    </Child>\\n  </Parent>\\n</App>\\n```\\n\\nIf `TargetComponent` needs a piece of data from `App`, you would have to pass that data through `Parent`, `Child`, and `Grandchild`, even though these components do not use the data themselves. This is known as prop drilling.\\n\\nFor more details, you can refer to the document \"W9-react\" on reference page 39.',\n",
       "  'messages': [AIMessage(content='Prop drilling in React refers to the process of passing data from a higher-level component to a lower-level component through multiple layers of intermediate components that do not use the data themselves. This can make the code more complex and harder to maintain.\\n\\n**Example:**\\n\\nConsider a component hierarchy like this:\\n\\n```\\n<App>\\n  <Parent>\\n    <Child>\\n      <Grandchild>\\n        <TargetComponent />\\n      </Grandchild>\\n    </Child>\\n  </Parent>\\n</App>\\n```\\n\\nIf `TargetComponent` needs a piece of data from `App`, you would have to pass that data through `Parent`, `Child`, and `Grandchild`, even though these components do not use the data themselves. This is known as prop drilling.\\n\\nFor more details, you can refer to the document \"W9-react\" on reference page 39.', additional_kwargs={}, response_metadata={})]}]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "list(agent_executor.stream({\"input\": \"btw what exactly do we mean by prop drelling? Can you give me an example...\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we now have an agent which will be answering the core question asked by the user, based on the knowledge base. We can replicate the same for Q&A knowledge base query, as that has a different type of dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
